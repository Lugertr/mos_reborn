from transformers import VisionEncoderDecoderModel, TrOCRProcessor, Seq2SeqTrainer, Seq2SeqTrainingArguments
from datasets import load_dataset
import torch
import os
from PIL import Image

# 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
model_name = "./trocr-base-handwritten-ru"  
processor = TrOCRProcessor.from_pretrained(model_name)
model = VisionEncoderDecoderModel.from_pretrained(model_name)

# 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
dataset = load_dataset(
    "json",
    data_files={
        "train": "data/handwritten/train.json",
        "test": "data/handwritten/test.json"
    }
)

# ‚ö° –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ –º–æ–∂–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–∏—Ç—å –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ
dataset["train"] = dataset["train"].select(range(200))
dataset["test"] = dataset["test"].select(range(50))

# 3. –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
def preprocess(batch):
    images = [Image.open(f"data/handwritten/images/{x}").convert("RGB") for x in batch["file_name"]]
    pixel_values = processor(images=images, return_tensors="pt", padding=True).pixel_values
    labels = processor.tokenizer(batch["text"], padding="max_length", truncation=True).input_ids
    labels = [[l if l != processor.tokenizer.pad_token_id else -100 for l in label] for label in labels]
    return {"pixel_values": pixel_values, "labels": labels}

dataset = dataset.map(preprocess, batched=True, remove_columns=dataset["train"].column_names)

# 4. –ê—Ä–≥—É–º–µ–Ω—Ç—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏
training_args = Seq2SeqTrainingArguments(
    output_dir="./checkpoints",         # üîπ –≤—Å–µ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã —Å—é–¥–∞
    per_device_train_batch_size=1,      # ‚Üì –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–∞—Ç—á (—ç–∫–æ–Ω–æ–º–∏—è VRAM)
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,      # –∏–º–∏—Ç–∞—Ü–∏—è –±–∞—Ç—á–∞ = 4
    predict_with_generate=True,
    logging_dir="./logs",
    logging_steps=50,
    num_train_epochs=3,                 # –Ω–∞—á–Ω—ë–º —Å 3, –ø–æ—Ç–æ–º –º–æ–∂–Ω–æ –ø–æ–¥–Ω—è—Ç—å
    save_strategy="steps",              # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤
    save_steps=200,
    save_total_limit=5,                 # —Ö—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5
    eval_strategy="steps",              # –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤
    eval_steps=200,
    fp16=True,                          # –ø–æ–ª–æ–≤–∏–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (—ç–∫–æ–Ω–æ–º–∏—Ç VRAM)
    resume_from_checkpoint=True         # –µ—Å–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å —É–ø–∞–¥—ë—Ç ‚Üí –ø—Ä–æ–¥–æ–ª–∂–∏—Ç
)

# 5. –¢—Ä–µ–Ω–µ—Ä
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    tokenizer=processor.feature_extractor,
)

# 6. –ó–∞–ø—É—Å–∫
trainer.train()

# 7. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Ç–æ–≥–æ–≤—É—é –º–æ–¥–µ–ª—å –æ—Ç–¥–µ–ª—å–Ω–æ
final_dir = "./trocr-finetuned"
os.makedirs(final_dir, exist_ok=True)
model.save_pretrained(final_dir)
processor.save_pretrained(final_dir)

print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ò—Ç–æ–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {final_dir}, —á–µ–∫–ø–æ–∏–Ω—Ç—ã –≤ ./checkpoints")
