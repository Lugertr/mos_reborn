# train_trocr.py
from transformers import VisionEncoderDecoderModel, TrOCRProcessor, Seq2SeqTrainer, Seq2SeqTrainingArguments
from datasets import load_dataset
from PIL import Image
import torch, os, re

# ========= 0. –£—Ç–∏–ª–∏—Ç—ã =========

def find_latest_checkpoint(base_dir: str) -> str | None:
    """
    –ù–∞—Ö–æ–¥–∏—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π ./checkpoints/checkpoint-XXXX.
    –í–µ—Ä–Ω—ë—Ç –ø—É—Ç—å –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —á–µ–∫–ø–æ–∏–Ω—Ç—É –∏–ª–∏ None, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç.
    """
    if not os.path.isdir(base_dir):
        return None
    best_num, best_path = -1, None
    for name in os.listdir(base_dir):
        m = re.fullmatch(r"checkpoint-(\d+)", name)
        if m:
            n = int(m.group(1))
            if n > best_num:
                best_num = n
                best_path = os.path.join(base_dir, name)
    return best_path

def simple_wer(ref: str, hyp: str) -> float:
    """
    –ü—Ä–æ—Å—Ç–µ–π—à–∏–π WER –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫.
    """
    r = ref.split()
    h = hyp.split()
    # –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω –ø–æ —Å–ª–æ–≤–∞–º
    dp = [[0]*(len(h)+1) for _ in range(len(r)+1)]
    for i in range(len(r)+1):
        dp[i][0] = i
    for j in range(len(h)+1):
        dp[0][j] = j
    for i in range(1, len(r)+1):
        for j in range(1, len(h)+1):
            cost = 0 if r[i-1] == h[j-1] else 1
            dp[i][j] = min(
                dp[i-1][j] + 1,      # deletion
                dp[i][j-1] + 1,      # insertion
                dp[i-1][j-1] + cost  # substitution
            )
    if len(r) == 0:
        return 0.0 if len(h) == 0 else 1.0
    return dp[len(r)][len(h)] / len(r)


# ========= 1. –ú–æ–¥–µ–ª—å/–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä =========

model_name = "./trocr-base-handwritten-ru"   # —Ç–≤–æ—è –ª–æ–∫–∞–ª—å–Ω–∞—è –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å
processor = TrOCRProcessor.from_pretrained(model_name)
model = VisionEncoderDecoderModel.from_pretrained(model_name)

# CPU-only? (—É —Ç–µ–±—è torch 2.8.0+cpu)
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# ========= 2. –î–∞—Ç–∞—Å–µ—Ç =========

dataset = load_dataset(
    "json",
    data_files={
        "train": "data/handwritten/train.json",
        "test":  "data/handwritten/test.json"
    }
)

# –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ–±–æ–ª—å—à–æ–π –ø–æ–¥–Ω–∞–±–æ—Ä
dataset["train"] = dataset["train"].select(range(200))
dataset["test"]  = dataset["test"].select(range(50))

# ========= 3. –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ =========

def preprocess(batch):
    images = [Image.open(f"data/handwritten/images/{x}").convert("RGB") for x in batch["file_name"]]
    pixel_values = processor(images=images, return_tensors="pt", padding=True).pixel_values
    # labels –∫–∞–∫ ids; –ø–∞–¥-–∏–¥ –∑–∞–º–µ–Ω—è–µ–º –Ω–∞ -100, —á—Ç–æ–±—ã –Ω–µ —É—á–∏—Ç—ã–≤–∞–ª—Å—è –≤ loss
    labels = processor.tokenizer(batch["text"], padding="max_length", truncation=True).input_ids
    labels = [[(l if l != processor.tokenizer.pad_token_id else -100) for l in lab] for lab in labels]
    return {"pixel_values": pixel_values, "labels": labels}

dataset = dataset.map(preprocess, batched=True, remove_columns=dataset["train"].column_names)

# ========= 4. –ê—Ä–≥—É–º–µ–Ω—Ç—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ =========

training_args = Seq2SeqTrainingArguments(
    output_dir="./checkpoints",
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,   # –∏–º–∏—Ç–∞—Ü–∏—è –±–∞—Ç—á–∞ = 4
    predict_with_generate=True,
    logging_dir="./logs",
    logging_steps=20,                # –ø–æ—á–∞—â–µ –ª–æ–≥–∏—Ä—É–µ–º –ª–æ—Å—Å
    num_train_epochs=3,
    save_strategy="steps",
    save_steps=200,
    save_total_limit=5,
    eval_strategy="steps",
    eval_steps=200,
    fp16=False,                      # —É —Ç–µ–±—è CPU, fp16 –æ—Ç–∫–ª—é—á–∞–µ–º
    remove_unused_columns=False,     # –≤–∞–∂–Ω–æ –¥–ª—è seq2seq + vision
)

# ========= 5. –ú–µ—Ç—Ä–∏–∫–∏ =========

def postprocess_text(pred_ids, label_ids):
    # —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º, —É–±–∏—Ä–∞–µ–º —Ç–æ–∫–µ–Ω—ã
    pred_str  = processor.batch_decode(pred_ids, skip_special_tokens=True)
    label_ids = [[l for l in ids if l != -100] for ids in label_ids]
    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)
    return pred_str, label_str

def compute_metrics(eval_pred):
    pred_ids, label_ids = eval_pred
    pred_str, label_str = postprocess_text(pred_ids, label_ids)
    wers = [simple_wer(ref, hyp) for ref, hyp in zip(label_str, pred_str)]
    # —Å—Ä–µ–¥–Ω–∏–π WER
    return {"wer": sum(wers)/len(wers) if wers else 0.0}

# ========= 6. –¢—Ä–µ–Ω–µ—Ä =========

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["test"],
    processing_class=processor,      # –≤–º–µ—Å—Ç–æ tokenizer (—É—Å—Ç–∞—Ä–µ–≤—à–∏–π)
    compute_metrics=compute_metrics,
)

# ========= 7. –ó–∞–ø—É—Å–∫ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ (–≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ, –µ—Å–ª–∏ –µ—Å—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç) =========

latest_ckpt = find_latest_checkpoint("./checkpoints")
if latest_ckpt:
    print(f"üîÑ –í–æ–∑–æ–±–Ω–æ–≤–ª—è–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å {latest_ckpt}")
    trainer.train(resume_from_checkpoint=latest_ckpt)
else:
    print("üöÄ –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è")
    trainer.train()

# ========= 8. –ò—Ç–æ–≥–æ–≤–∞—è —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω–∞—è –º–æ–¥–µ–ª—å =========

final_dir = "./trocr-finetuned"
os.makedirs(final_dir, exist_ok=True)
model.save_pretrained(final_dir)
processor.save_pretrained(final_dir)
print(f"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ò—Ç–æ–≥–æ–≤–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {final_dir}, —á–µ–∫–ø–æ–∏–Ω—Ç—ã –≤ ./checkpoints")
