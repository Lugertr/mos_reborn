"""
Человеко-читаемый конфиг для TrOCR с подробными комментариями и профилями PRESETS.

ЗАЧЕМ НУЖЕН ЭТОТ ФАЙЛ
--------------------
1) Все значения в одном месте с комментариями — удобнее, чем "немой" JSON.
2) Профили (PRESETS) — быстрые наборы оверрайдов под разные сценарии (база/архивы/машинопечать).
3) Скрипт обучения читает CONFIG и при желании накладывает профиль (--profile),
   а CLI-флаги поверх перекрывают и CONFIG, и профиль (максимальный приоритет).

ПРИОРИТЕТЫ НАСТРОЕК (от высокого к низкому)
-------------------------------------------
1) CLI флаги:      то, что вы явно передали в командной строке (например, --epochs 6).
2) --profile:      выбранный профиль из PRESETS накладывается поверх CONFIG.
3) CONFIG:         базовые значения из словаря ниже.
4) Значения по умолчанию в коде (датакласс TrainConfig).

ВАЖНО О РАЗМЕРАХ ИЗОБРАЖЕНИЙ
----------------------------
Вы заранее сегментируете строки до ~120x200 (высота=120, ширина=200).
Эти размеры должны совпадать с конфигом (img_h/img_w), иначе будут ошибки размеров или падение качества.

МЕТРИКИ
-------
- CER (Character Error Rate): доля ошибочных символов; 0.085 = 8.5% ошибок. Ниже — лучше.
- WER (Word Error Rate): доля ошибочных слов. Ниже — лучше.
Рекомендация: monitor_metric="cer" для кириллицы/дореформенной орфографии.

РЕКОМЕНДАЦИИ ПО ГИПЕРПАРАМЕТРАМ
-------------------------------
Базовая тренировка (≈72k):
- epochs: 30–50 (40 по умолчанию нормально, включена ранняя остановка)
- learning_rate: 2e-4
- label_smoothing: 0.10
- lr_decay_rate: 0.98 (мягкое затухание)
- early_stop_patience: 8–10 (в этом конфиге 6 — можно поднять до 8)

Дообучения (сотни–пара тысяч):
- learning_rate: 3e-5…7e-5 (если encoder заморожен — можно 5e-5…1e-4)
- epochs: 4–10 (обычно 6–8)
- label_smoothing: 0.12 при шуме/мало данных
- lr_decay_rate: 0.98–0.995 (чем меньше данных — тем ближе к 0.995)
- early_stop_patience: 4–6

КАК ИСПОЛЬЗОВАТЬ
----------------
- База (72k):
    python -m trocr.trocr_modified train --run-dir base72k --subset full \
        --config trocr/trocr_config.py --profile base_72k
- Дообучение на архивах:
    python -m trocr.trocr_modified finetune --base-run base72k --new-run ft_archiveset1 \
        --subset postTest --weights best.weights.h5 \
        --config trocr/trocr_config.py --profile archives_adapt

"""

CONFIG = {
    # --- Пути и структура набора ---
    # Корень датасета со всеми подпапками ('train', 'test', 'postTest') и JSONами.
    "data_root": "data/handwritten",

    # --- Геометрия входных изображений ---
    # Должна совпадать с вашей предварительной сегментацией.
    "img_h": 120,     # высота в пикселях (масштабирование по высоте обязательно)
    "img_w": 200,     # ширина в пикселях (строка справа будет дополняться белым или обрезаться)

    # --- Ограничение длины целевого текста ---
    # Максимум токенов, которые модель видит на выходе (внутри пайплайна добавляются <s> и </s>).
    # Если 95-й перцентиль длины строк больше — поднимите (цена: память/время).
    "max_text_len": 80,

    # --- Пакет и базовое расписание обучения ---
    "batch_size": 32,       # уменьшайте до 24/16 при нехватке VRAM; увеличивайте при большой GPU
    "epochs": 40,           # верхний предел; ранняя остановка завершит раньше
    "learning_rate": 2e-4,  # стартовый LR для базы

    # --- Регуляризация и архитектура ---
    "label_smoothing": 0.10,  # 0.05 — очень чисто; 0.10 — базово; 0.12–0.15 — мало/шумно
    "d_model": 256,
    "num_heads": 8,
    "dff": 512,
    "dec_layers": 4,
    "enc_dropout": 0.10,
    "dec_dropout": 0.10,

    # --- Мониторинг и точность ---
    "monitor_metric": "wer",  # "cer" или "wer"; рекомендуем "cer"
    "mixed_precision": True, # True — быстрее/экономней на Tensor Cores (Ampere/ADA)

    # --- Затухание шага и ранняя остановка ---
    "lr_decay_rate": 0.98,    # LR *= 0.98 каждый эпох; 1.0 — отключить
    "early_stop_patience": 6, # эпох терпеть отсутствие улучшения val_loss

    # --- Валидационный отчёт EvalCallback ---
    "val_batches_for_eval": 8, # сколько батчей брать из валидации для отчёта CER/WER

    # --- Воспроизводимость ---
    "seed": 42
}


# Профили (накладываются поверх CONFIG; CLI перекрывает оба)
PRESETS = {
    # БАЗОВОЕ ОБУЧЕНИЕ (~72k образцов)
    "base_72k": {
        "learning_rate": 2e-4,
        "epochs": 40,
        "early_stop_patience": 8,
        "lr_decay_rate": 0.98,
        "label_smoothing": 0.10,
        "monitor_metric": "cer"
    },

    # МАЛО НОВЫХ ДАННЫХ, ДОМЕН БЛИЗКИЙ (заморозьте энкодер через --freeze-encoder)
    "finetune_small_frozen": {
        "learning_rate": 5e-5,
        "epochs": 8,
        "early_stop_patience": 5,
        "label_smoothing": 0.12,
        "lr_decay_rate": 0.99
    },

    # ЯВНЫЙ ДОМЕННЫЙ СДВИГ (архивные фрагменты), обучаем всё (без заморозки)
    "archives_adapt": {
        "learning_rate": 3e-5,
        "epochs": 6,
        "early_stop_patience": 4,
        "label_smoothing": 0.12,
        "lr_decay_rate": 0.995
    },

    # МАШИНОПЕЧАТНЫЙ, ЧИСТЫЙ (меньше сглаживания, умеренное затухание)
    "typewritten_clean": {
        "learning_rate": 1e-4,
        "epochs": 20,
        "early_stop_patience": 6,
        "label_smoothing": 0.05,
        "lr_decay_rate": 0.98
    }
}
